# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
## Abstract
* unlabeled text에서 right, left context를 모두 고려하는 deep bidirectional representations을 pre-train시켰다.
* 이를 통해 pre-trained BERT 모델을 fine tuning 했더니 SOTA.
* task-specific한 구조를 디자인 할 필요 없다.

## 1. Introduction
* pre-trained language representation을 task에 적용하는 두 가지 방법이 있다.
    * 두 가지 모두 unidirectional language model
    1. feature based: ELMo와 같은 모델이 이에 속한다.
    2. fine-tuning: GPT와 같은 모델이 이에 속한다. task-specific한 parameters를 최대한 줄이고, pretrained parameters를 fine-tuning하기만 하는 방법이다.

* BERT에서 masked LM을 사용했는데, 이는 입력 문장에서 임의로 특정 토큰을 가리고 해당 문장만을 주고 가려진 토큰를 추측하게 만드는 방법이다.
    * masked LM은 기존의 LM과는 다르게 양쪽의 context를 전부 활용할 수 있다.

## 2. Related Work
생략

## 3. BERT
![BERT](https://user-images.githubusercontent.com/79077316/127760462-f973cbfe-0635-48f2-b235-f8f260e379ef.png)
> pre-training 후 fine-tuning하는 2 스텝으로 이루어졌다.
* pre-training
    * unlabeled data로 학습한다.
* fine-tuning
    * pre-training에서 학습한 모델 parameter로 초기화한 다음에 labeled data로 다시 학습한다.

#### Model Architecture
* BERT의 네트워크 구조는 transformer를 deep하게 쌓은 것이다.
* Multi-layer bidirectional Transformer encoder
* 다른 task에서도 전부 통일된 아키텍처를 가지고 있다.
* 2개의 네트워크 구조를 갖는다.
    * BERT-base: L=12, H=768, A=12, Total Parameters=110M
    * BERT-large: L=24, H=1024, A=16, Total Parameters=340M

#### Input Output Representation
![BERT-input representation](https://user-images.githubusercontent.com/79077316/127761936-e0a1bc89-055e-40a1-8875-82627ce041c0.png)
> BERT의 input은 위와 같이 3가지 embedding 값의 합으로 이루어진다.
1. Token embedding - 토큰의 의미 표현
* 30,000개의 단어로 WordPiece embedding을 사용했다.
* 문장의 시작을 알리는 토큰은 `[CLS]`로 나타낸다.
* 다른 문장이 들어오면 `[SEP]`를 사용한다.

2. Segment embedding - 문장과 문장을 이어주는 용도

3. Position embedding - 토큰의 sequential한 의미를 가짐

### 3-1. Pre-training BERT
BERT는 기존의 ELMo나 GPT와 다르게 2가지의 새로운 unsupervised prediction task로 pre-training을 수행한다.

#### Task1. Masked LM
* Masked Language Model(MSM)은 input에서 랜덤하게 몇 개의 토큰을 mask시킨다. 이를 transformer에 넣어서 주변 단어의 context만을 보고 mask된 단어를 예측한다.
* input 전체와 mask된 token을 한번에 Transformer encoder에 넣고 원래 token 값을 예측하므로 deep bidirectional 하다.
* 단어 중의 일부를 `[MASK]` token으로 바꾼다. 바꾸는 비율은 15%이다.
* `[MASK]` token은 pre-training에만 사용되고, fine-tuning시에는 사용되지 않는다. 
    * 해당 token을 맞추어 내는 task를 수행하면서 BERT는 문맥을 파악하는 능력을 기를 수 있다.

#### Task2. Next Sentence Prediction(NSP)
* BERT에서는 corpus에서 두 문장을 이어 붙여 이것이 원래의 corpus에서 바로 이어 붙여져 있던 문장인지를 맞추는 binarized next sentence prediction task를 수행한다.
    * 두 문장을 pre-training시에 같이 넣어줘서 두 문장이 이어지는 문장인지 아닌지 맞추는 task이다.
* pre-training시에는 50:50 비율로 실제로 이어지는 두 문장과 랜덤하게 추출된 두 문장을 넣어줘서 BERT가 맞추게 한다.
* QZ, NLI task에서 특히 도움이 많이 되었다. 

### 3-2. Fine-tuning BERT
* task-specific input과 output을 BERT에 넣어주고 end-to-end로 fine-tuning해준다.

## 4. Experiments
생략